# Foundation Model Pre-training Configuration
# Multi-Scale Fusion Backbone for CRC Analysis

model:
  name: "MultiScaleFusionBackbone"
  architecture:
    base_encoder: "resnet50"  # Options: resnet50, resnet101, efficientnet_b0-b7, vit_base, vit_large, convnext_base
    scales: [1.0, 0.5, 0.25, 0.125]  # Added 0.125 scale for broader tissue context
    feature_dim: 512
    projection_dim: 256  # For contrastive learning
    num_classes: 128  # For prototype learning
    
  multiscale_fusion:
    use_fpn: true
    fpn_channels: 256
    use_cross_scale_attention: true
    attention_heads: 8
    use_scale_aware_pooling: true
    scale_dropout: 0.1  # Dropout for scale robustness
    
  pretrained_weights: null  # Start from scratch for better domain adaptation

datasets:
  tcga:
    enabled: true
    path: "/data/tcga_colorectal"
    num_samples: 50000
    patch_size: 256
    stride: 128
    magnification: "20x"
    stain_normalization: "macenko"
    
  camelyon:
    enabled: true
    path: "/data/camelyon16"
    num_samples: 30000
    patch_size: 256
    stride: 128
    magnification: "20x"
    
  internal:
    enabled: false  # Enable when available
    path: "/data/internal_slides"
    num_samples: 20000
    patch_size: 256
    
pretraining:
  # Phase 1: MAE (Masked Autoencoder)
  mae:
    enabled: true
    epochs: 100
    mask_ratio: 0.75
    decoder_depth: 8
    decoder_dim: 512
    patch_size: 16
    reconstruction_loss: "l2"
    warmup_epochs: 10
    
  # Phase 2: SimCLR (Contrastive Learning)
  simclr:
    enabled: true
    epochs: 200
    temperature: 0.07
    projection_head_dims: [2048, 2048, 256]
    augmentations:
      color_jitter_strength: 0.5
      gaussian_blur_prob: 0.5
      random_grayscale_prob: 0.2
      rotation_degrees: 45
      elastic_transform: true
      
  # Phase 3: DINO (Self-Distillation)
  dino:
    enabled: true
    epochs: 100
    student_temp: 0.1
    teacher_temp: 0.07
    teacher_momentum: 0.996
    warmup_teacher_temp_epochs: 30
    local_crops_number: 8
    
  # Phase 4: MoCo v3 (Momentum Contrast)
  moco:
    enabled: true
    epochs: 100
    temperature: 0.2
    momentum: 0.999
    queue_size: 65536
    symmetric_loss: true

augmentations:
  spatial:
    random_flip: true
    random_rotation: [-45, 45]  # Explicit rotation range
    random_scale: [0.8, 1.2]
    elastic_deformation:
      alpha: 120
      sigma: 9
    use_rotation: true  # Enable rotation augmentation
      
  color:
    h_shift: 0.05
    s_shift: 0.05
    v_shift: 0.05
    color_jitter:
      brightness: 0.4
      contrast: 0.4
      saturation: 0.4
      hue: 0.1
      
  advanced:
    mixup_alpha: 0.2
    cutmix_alpha: 1.0
    stain_augmentation: true
    nucleus_augmentation: true
    use_mixup: true  # Enable mixup for fine-tuning
    use_cutmix: true  # Enable cutmix for fine-tuning

training:
  batch_size: 256  # Adjust based on GPU memory
  num_workers: 16
  learning_rate: 1e-3
  weight_decay: 1e-5
  optimizer: "adamw"
  scheduler:
    type: "cosine"
    warmup_epochs: 10
    min_lr: 1e-6
    
  gradient_clip: 5.0
  mixed_precision: true
  distributed:
    enabled: true
    backend: "nccl"
    
  checkpointing:
    save_frequency: 10  # epochs
    keep_last_k: 5
    save_best: true
    
validation:
  downstream_tasks:
    - "tissue_classification"
    - "molecular_subtyping" 
    - "metastasis_detection"
  validation_frequency: 10  # epochs
  metrics:
    - "knn_accuracy"
    - "linear_probe_accuracy"
    - "feature_quality"

hardware:
  gpus: 4  # Number of GPUs for distributed training
  gpu_type: "A100"  # or V100, RTX3090, etc.
  memory_per_gpu: "40GB"
  
logging:
  wandb:
    enabled: true
    project: "crc_foundation_model"
    entity: "medical_ai_lab"
  tensorboard:
    enabled: true
    log_dir: "./logs/tensorboard"
  log_frequency: 100  # iterations

output:
  checkpoint_dir: "./foundation_model/checkpoints"
  log_dir: "./foundation_model/logs"
  visualization_dir: "./foundation_model/visualizations"

# New section for fine-tuning configuration
finetuning:
  # Molecular subtype prediction settings
  molecular_subtyping:
    num_classes: 3  # Canonical, Immune, Stromal (Pitroda classification)
    classifier_type: "mlp"  # Options: "linear", "mlp"
    mlp_hidden_dims: [256]  # Hidden dimensions for MLP classifier
    dropout: 0.3
    
  # Learning rate settings
  differential_lr:
    enabled: true
    backbone_lr: 1e-4  # Lower LR for pretrained backbone
    classifier_lr: 1e-3  # Higher LR for new classifier head
    
  # Gradual unfreezing
  gradual_unfreezing:
    enabled: true
    freeze_epochs: 2  # Epochs to train only classifier
    unfreeze_schedule: [5, 10]  # Epochs to unfreeze last block, then all
    
  # Semi-supervised learning
  semi_supervised:
    enabled: true
    consistency_weight: 0.1  # Weight for consistency loss
    use_mean_teacher: false  # Alternative to consistency regularization
    
  # Class balancing
  class_balancing:
    enabled: true
    method: "weighted_loss"  # Options: "weighted_loss", "oversampling", "focal_loss"
    focal_loss_gamma: 2.0
    
  # Training settings
  batch_size: 32  # Smaller batch for fine-tuning
  epochs: 50
  early_stopping:
    enabled: true
    patience: 10
    monitor: "val_f1_macro"  # Monitor macro F1 score
    
  # Evaluation metrics
  metrics:
    - "accuracy"
    - "auc_per_class"
    - "f1_macro"
    - "f1_per_class"
    - "confusion_matrix"
    - "mcc"  # Matthews correlation coefficient

# Clinical metadata integration
clinical_integration:
  enabled: true
  features:
    - "age"
    - "sex"
    - "tumor_location"
    - "grade"
    - "msi_status"
    - "stage"
  embedding_dims:
    categorical: 16  # Embedding dimension for categorical features
    numerical: 1  # Direct use of numerical features
  fusion_method: "concatenation"  # Options: "concatenation", "attention", "gated" 