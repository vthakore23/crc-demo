# CRC Molecular Subtype Classification - UChicago RCC Configuration
# University of Chicago Research Computing Center

# Data Configuration
data:
  # Root directory for all data (update with your username and cluster)
  # For Midway2: /scratch/midway2/username/crc_molecular_training/data
  # For Randi: /scratch/username/crc_molecular_training/data
  root_dir: "/scratch/[cluster]/username/crc_molecular_training/data"
  
  # EPOC WSI data paths
  raw_data_dir: "/scratch/[cluster]/username/crc_molecular_training/data/raw"
  processed_data_dir: "/scratch/[cluster]/username/crc_molecular_training/data/processed"
  manifest_file: "/scratch/[cluster]/username/crc_molecular_training/data/manifests/epoc_manifest.csv"
  
  # Output directories
  output_dir: "/scratch/[cluster]/username/crc_molecular_training/results"
  checkpoint_dir: "/scratch/[cluster]/username/crc_molecular_training/models/checkpoints"
  log_dir: "/scratch/[cluster]/username/crc_molecular_training/logs"

# Model Configuration
model:
  # Architecture type
  architecture: "enhanced_molecular_predictor"
  
  # Model parameters
  num_classes: 3  # Canonical, Immune, Stromal
  input_size: 256
  dropout_rate: 0.2
  
  # Ensemble configuration
  use_ensemble: true
  ensemble_models:
    - "swin_transformer"
    - "convnext"
    - "efficientnet"

# Training Configuration
training:
  # Basic parameters
  batch_size: 32           # Per GPU batch size (adjust based on GPU memory)
  num_epochs: 100
  learning_rate: 1e-4
  weight_decay: 1e-5
  
  # Optimization
  optimizer: "adamw"
  scheduler: "cosine"
  warmup_epochs: 10
  
  # Data loading
  num_workers: 8           # Per GPU workers (adjust based on CPU cores)
  pin_memory: true
  persistent_workers: true
  
  # Augmentation
  use_augmentation: true
  augmentation_strength: 0.5
  
  # Validation
  validation_split: 0.2
  validation_frequency: 5  # Epochs between validation

# Hardware Configuration (RCC Infrastructure)
hardware:
  # Cluster selection
  cluster: "midway2"       # Options: "midway2", "randi"
  
  # CPU configuration (for Midway2)
  cpu_type: "skylake"      # Options: "broadwl" (28 cores), "skylake" (40 cores)
  nodes: 1                 # Number of nodes
  cpus_per_task: 40        # CPU cores per node (28 for broadwl, 40 for skylake)
  memory_per_node: "96G"   # Memory allocation (64G for broadwl, 96G for skylake)
  
  # GPU configuration (if using GPU nodes)
  use_gpu: false           # Set to true for GPU training
  num_gpus: 4              # Number of GPUs (4 K80s per node on Midway2, or A100s on Randi)
  gpu_type: "k80"          # Options: "k80" (Midway2), "a100" (Randi)
  
  # Distributed training
  distributed: false       # Enable for multi-node training
  backend: "nccl"          # NCCL for GPU, gloo for CPU
  
  # Mixed precision (GPU only)
  use_amp: false           # Automatic Mixed Precision (enable for GPU training)
  amp_level: "O1"

# Monitoring Configuration
monitoring:
  # Checkpointing
  save_frequency: 5        # Save every N epochs
  keep_last_n: 5          # Keep last N checkpoints
  
  # Logging
  log_frequency: 100       # Log every N steps
  tensorboard: true
  wandb: false             # Set to true if using Weights & Biases
  
  # Early stopping
  early_stopping: true
  patience: 15             # Epochs without improvement
  min_delta: 0.001         # Minimum improvement threshold

# Performance Configuration
performance:
  # Memory optimization
  gradient_checkpointing: false  # Enable if memory constrained
  max_memory_usage: 0.9          # Maximum GPU memory usage
  
  # Speed optimization
  compile_model: false           # PyTorch 2.0 compilation (disable for older PyTorch)
  channels_last: false           # Memory format optimization (GPU only)
  
  # Data pipeline
  prefetch_factor: 2
  multiprocessing_context: "spawn"

# Paths (RCC-specific)
paths:
  # Scratch space (high-performance storage)
  scratch_dir: "/scratch/[cluster]/username/crc_molecular_training"
  
  # Temporary directory
  temp_dir: "/tmp/crc_training_${SLURM_JOB_ID}"
  
  # Module paths
  python_path: "/scratch/[cluster]/username/crc_molecular_training"

# SLURM Integration
slurm:
  # Job configuration
  partition: "skylake"       # Options: "broadwl", "skylake", "gpu", "bigmem"
  account: "pi-username"     # Update with your PI account
  time_limit: "48:00:00"     # 48 hours
  
  # Resource requests (adjust based on cluster and partition)
  gres: ""                   # "gpu:4" for GPU nodes, "" for CPU nodes
  constraint: ""             # "skylake" for specific CPU type, "a100" for specific GPU
  
  # Notifications
  mail_type: "BEGIN,END,FAIL"
  mail_user: "username@uchicago.edu"

# Environment Configuration
environment:
  # Required modules (adjust based on available modules)
  modules:
    - "cuda/11.8"          # Only if using GPU
    - "python/3.11"
    - "gcc/11.2.0"
    - "openmpi/4.1.4"      # For distributed training
  
  # Environment variables
  env_vars:
    CUDA_VISIBLE_DEVICES: "0,1,2,3"  # Adjust based on available GPUs
    OMP_NUM_THREADS: "8"             # Adjust based on CPU cores
    NCCL_DEBUG: "INFO"               # For GPU debugging
    PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:512"

# Validation Configuration
validation:
  # Metrics to compute
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1_score"
    - "auc"
  
  # Test-time augmentation
  tta_enabled: true
  tta_steps: 8
  
  # Uncertainty quantification
  uncertainty_enabled: true
  mc_dropout_samples: 100

# Clinical Configuration
clinical:
  # Molecular subtypes
  class_names:
    - "Canonical"
    - "Immune" 
    - "Stromal"
  
  # Class weights (if imbalanced)
  use_class_weights: true
  
  # Clinical metrics
  survival_analysis: false  # Enable if survival data available
  
# Debug Configuration
debug:
  # Debug mode
  enabled: false
  
  # Profiling
  profile_memory: false
  profile_compute: false
  
  # Reduced dataset for testing
  use_subset: false
  subset_size: 1000

# Cluster-Specific Configurations
cluster_configs:
  midway2:
    # Midway2 specific settings
    partitions: ["broadwl", "skylake", "gpu", "bigmem"]
    max_time: "36:00:00"
    max_nodes: 10
    storage_path: "/scratch/midway2"
    
  randi:
    # Randi specific settings
    partitions: ["gpu"]
    max_time: "168:00:00"  # 7 days
    max_nodes: 2
    storage_path: "/scratch" 